{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Adapted from CS109a Introduction to Data Science\n",
    "## Seminar 8, Exercise 3: Bagging Classification with Decision Boundary\n",
    "\n",
    "## Description :\n",
    "The goal of this exercise is to use **Bagging** (Bootstrap Aggregated) to solve a classification problem and visualize the influence on Bagging on trees with varying depths.\n",
    "\n",
    "## Instructions:\n",
    "\n",
    "- Read the dataset `Crimes_-_Maps.csv`.\n",
    "- Assign the predictor and response variables as `X` and `y`.\n",
    "- Split the data into train and test sets with `test_split=0.2` and `random_state=44`.\n",
    "- Fit a single `DecisionTreeClassifier()` and find the accuracy of your prediction.\n",
    "- Complete the helper function `prediction_by_bagging()` to find the average predictions for a given number of bootstraps.\n",
    "- Perform `Bagging` using the helper function, and compute the new accuracy.\n",
    "- Plot the accuracy as a function of the number of bootstraps.\n",
    "- Use the helper code to plot the decision boundaries for varying max_depth along with `num_bootstraps`. Investigate the effect of increasing bootstraps on the variance.\n",
    "\n",
    "## Hints:\n",
    "\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn-tree-decisiontreeclassifier\" target=\"_blank\">sklearn.tree.DecisionTreeClassifier()</a>\n",
    "A decision tree classifier.\n",
    "\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.fit\" target=\"_blank\">DecisionTreeClassifier.fit()</a>\n",
    "Build a decision tree classifier from the training set (X, y).\n",
    "\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.predict\" target=\"_blank\">DecisionTreeClassifier.predict()</a>\n",
    "Predict class or regression value for X.\n",
    "\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\" target=\"_blank\">train_test_split()</a>\n",
    "Split arrays or matrices into random train and test subsets.\n",
    "\n",
    "<a href=\"https://docs.scipy.org/doc//numpy-1.10.4/reference/generated/numpy.random.choice.html\" target=\"_blank\">np.random.choice</a>\n",
    "Generates a random sample from a given 1-D array.\n",
    "\n",
    "<a href=\"https://matplotlib.org/3.2.1/api/_as_gen/matplotlib.pyplot.subplots.html\" target=\"_blank\">plt.subplots()</a>\n",
    "Create a figure and a set of subplots.\n",
    "\n",
    "<a href=\"https://matplotlib.org/3.2.1/api/_as_gen/matplotlib.axes.Axes.plot.html\" target=\"_blank\">ax.plot()</a>\n",
    "Plot y versus x as lines and/or markers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import scipy.optimize as opt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Used for plotting later\n",
    "from matplotlib.colors import ListedColormap\n",
    "cmap_bold = ListedColormap(['#F7345E','#80C3BD'])\n",
    "cmap_light = ListedColormap(['#FFF4E5','#D2E3EF'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read the file 'Crimes_-_Map.csv' as a Pandas dataframe\n",
    "df = pd.read_csv('data/Crimes_-_Map.csv')\n",
    "\n",
    "# Take a quick look at the data\n",
    "# Note that the LATITUDE & LONGITUDE values are not normalized\n",
    "df.head()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set the values of LATITUDE & LONGITUDE predictor variables\n",
    "X = ___.values\n",
    "\n",
    "# Use the column \"ARREST\" as the response variable\n",
    "y = ___.values\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split data in train an test, with test size = 0.2\n",
    "# and set random state as 44\n",
    "X_train, X_test, y_train, y_test = ___\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the max_depth of the decision tree\n",
    "max_depth = ___\n",
    "\n",
    "# Define a decision tree classifier with a max depth as defined above\n",
    "# and set the random_state as 44\n",
    "clf = ___\n",
    "\n",
    "# Fit the model on the training data\n",
    "___\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use the trained model to predict on the test set\n",
    "prediction = ___\n",
    "\n",
    "# Calculate the accuracy of the test predictions of a single tree\n",
    "single_acc = ___\n",
    "\n",
    "# Print the accuracy of the tree\n",
    "print(f'Single tree Accuracy is {single_acc*100}%')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Complete the function below to get the prediction by bagging\n",
    "\n",
    "# Inputs: X_train, y_train to train your data\n",
    "# X_to_evaluate: Samples that you are goin to predict (evaluate)\n",
    "# num_bootstraps: how many trees you want to train\n",
    "# Output: An array of predicted classes for X_to_evaluate\n",
    "\n",
    "def prediction_by_bagging(X_train, y_train, X_to_evaluate, num_bootstraps):\n",
    "\n",
    "    # List to store every array of predictions\n",
    "    predictions = []\n",
    "\n",
    "    # Generate num_bootstraps number of trees\n",
    "    for i in range(num_bootstraps):\n",
    "\n",
    "        # Sample data to perform first bootstrap, here, we actually bootstrap indices,\n",
    "        # because we want the same subset for X_train and y_train\n",
    "        resample_indexes = np.random.choice(np.arange(y_train.shape[0]), size=y_train.shape[0])\n",
    "\n",
    "        # Get a bootstrapped version of the data using the above indices\n",
    "        X_boot = X_train[___]\n",
    "        y_boot = y_train[___]\n",
    "\n",
    "        # Initialize a Decision Tree on bootstrapped data\n",
    "        # Use the same max_depth and random_state as above\n",
    "        clf = ___\n",
    "\n",
    "        # Fit the model on bootstrapped training set\n",
    "        clf.fit(___,___)\n",
    "\n",
    "        # Use the trained model to predict on X_to_evaluate samples\n",
    "        pred = clf.predict(___)\n",
    "\n",
    "        # Append the predictions to the predictions list\n",
    "        predictions.append(pred)\n",
    "\n",
    "    # The list \"predictions\" has [prediction_array_0, prediction_array_1, ..., prediction_array_n]\n",
    "    # To get the majority vote for each sample, we can find the average\n",
    "    # prediction and threshold them by 0.5\n",
    "    average_prediction = ___\n",
    "\n",
    "    # Return the average prediction\n",
    "    return average_prediction\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the number of bootstraps\n",
    "num_bootstraps = 20\n",
    "\n",
    "# Calling the prediction_by_bagging function with appropriate parameters\n",
    "y_pred = prediction_by_bagging(X_train,y_train,X_test,num_bootstraps=num_bootstraps)\n",
    "\n",
    "# Compare the average predictions to the true test set values\n",
    "# and compute the accuracy\n",
    "bagging_accuracy = ___\n",
    "\n",
    "# Print the bagging accuracy\n",
    "print(f'Accuracy with Bootstrapped Aggregation is  {bagging_accuracy*100}%')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mindchow üç≤\n",
    "Play around with the following parameters:\n",
    "\n",
    "- max_depth\n",
    "- numboot\n",
    "\n",
    "Based on your observations, answer the questions below:\n",
    "\n",
    "- How does the plot change with varying `max_depth`\n",
    "\n",
    "- How does the plot change with varying `numboot`\n",
    "\n",
    "- How are the three plots essentially different?\n",
    "\n",
    "- Does more bootstraps reduce overfitting for\n",
    "    - High depth\n",
    "    - Low depth"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
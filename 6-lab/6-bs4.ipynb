{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Adapted from CS109a Introduction to Data Science\n",
    "\n",
    "slides are available [here](https://docs.google.com/presentation/d/1EaZWSQBCAUyVgXZbvOFBiVWpSL25yx8O90j96p2DlJA/edit?usp=sharing)\n",
    "\n",
    "## Seminar 6, Exercise 1: Intro to BS4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Description\n",
    "\n",
    "**OVERVIEW**\n",
    "\n",
    "As we learned in class, the three most common sources of data used for Data Science are:\n",
    "\n",
    "- files (e.g, `.csv`, `.txt`) that already contain the dataset\n",
    "- APIs (e.g., Twitter or Facebook)\n",
    "- web scraping (e.g., Requests)\n",
    "\n",
    "Here, we get practice with web scraping by using **Requests**. Once we fetch the page contents, we will need to extract the information that we actually care about. We rely on <a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\" target=\"_blank\">BeautifulSoup</a> to help with this."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For this exercise, we will be grabbing data (the Top News stories) from [AP News](apnews.com), a not-for-profit news agency."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# the URL of the webpage that has the desired info\n",
    "url = \"https://apnews.com/hub/ap-top-news\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's use [`requests`](https://requests.readthedocs.io/en/master/user/quickstart/) to fetch the contents. Specifically, the [`requests`](https://requests.readthedocs.io/en/master/user/quickstart/) library has a `.get()` function that returns a [Response object](https://www.w3schools.com/python/ref_requests_response.asp). A Response object contains the server's _response_ to the HTTP request, and thus contains all the information that we could want from the page.\n",
    "\n",
    "Below, fill in the blank to fetch  AP News' Top News website."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "home_page = requests.get(____)\n",
    "home_page.status_code"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You should have received a status code of 200, which means the page was successfully found on the server and sent to receiver (aka client/user/you). [Again, you can click here](https://www.restapitutorial.com/httpstatuscodes.html) for a full list of status codes. Recall that sometimes, while browsing the Internet, webpages will report a 404 error, possibly with an entertaining graphic to ease your pain. That 404 is the status code, just like we are using here!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`home_page` is now a [Response object](https://www.w3schools.com/python/ref_requests_response.asp). It contains many attributes, including the `.text`. Run the cell below and note that it's identical to if we were to visit the webpage in our browser and clicked 'View Page Source'."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "home_page.text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above `.text` property is atrocious to view and make sense of. Sure, we could write Regular Expressions to extract all of the contents that we're interested in. Instead, let's first use [`BeautifulSoup`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) to parse the content into more manageable chunks.\n",
    "\n",
    "Below, fill in the blank to construct an HTML-parsed [`BeautifulSoup`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) object from our website."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '____' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [3]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m soup \u001B[38;5;241m=\u001B[39m BeautifulSoup(\u001B[43m____\u001B[49m, ____)\n\u001B[0;32m      2\u001B[0m soup\n",
      "\u001B[1;31mNameError\u001B[0m: name '____' is not defined"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(____, ____)\n",
    "soup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You'll notice that the `soup` object is better formatted than just looking at the entire text. It's still dense, but it helps.\n",
    "\n",
    "Below, fill in the blank to set `webpage_title` equal to the text of the webpage's title (no HTML tags included)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "webpage_title = ____"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again, our BeautifulSoup object allows for quick, convenient searching and access to the web page contents.\n",
    "\n",
    "\n",
    "Anytime you wish to extract specific contents from a webpage, it is necessary to:\n",
    "- **Step 1**. While viewing the page in your browser, identify what contents of the page you're interested in.\n",
    "- **Step 2**. Look at the HTML returned from the BeautifulSoup object, and pinpoint the specific context that surrounds each of these items that you're interested in\n",
    "- **Step 3.** Devise a pattern using BeautifulSoup and/or RegularExpressions to extract said contents."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For example:\n",
    "### **Step 1:**\n",
    "Let's say, for every news article found on the AP's Top News page, you want to extract the link and associated title. In this screenshot\n",
    "<img src=\"https://github.com/Harvard-IACS/2020-CS109A/blob/master/content/lectures/lecture03/images/apnews_sample.png?raw=true\">\n",
    "\n",
    "we can see one news article (there are many more below on the page). Its title is `\"California fires bring more chopper rescues, power shutoffs\"` and its link is to [/c0aa17fff978e9c4768ee32679b8555c](/c0aa17fff978e9c4768ee32679b8555c). Since the current page is stored at apnews.com, the article link's full address is [apnews.com/c0aa17fff978e9c4768ee32679b8555c](apnews.com/c0aa17fff978e9c4768ee32679b8555c).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### **Step 2:**\n",
    "\n",
    "After printing the `soup` object, we saw a huge mess of all of the HTML still. So, let's drill down into certain sections. As illustrated in the [official documentation here](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#navigating-using-tag-names), we can retrieve all `<a>` links by running the cell below:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "soup.find_all(\"a\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is still a ton of text (links). So, let's get more specific. I now search for the title text `California fires bring more chopper rescues, power shutoffs` within the output of the previous cell (the HTML of all links). I notice the following:\n",
    "\n",
    "`<a class=\"Component-headline-0-2-110\" data-key=\"card-headline\" href=\"/c0aa17fff978e9c4768ee32679b8555c\"><h1 class=\"Component-h1-0-2-111\">California fires bring more chopper rescues, power shutoffs</h1></a>`\n",
    "\n",
    "I also see that this is repeatable; every news article on the Top News page has such text! Great!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Step 3:**\n",
    "\n",
    "The pattern is that we want the value of the `href` attribute, along with the text of the link. There are many ways to get at this information. Below, I show just a few:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [4]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# EXAMPLE 1\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# returns all `a` links that also contain `Component-headline-0-2-110`\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m \u001B[43msoup\u001B[49m\u001B[38;5;241m.\u001B[39mfind_all(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ma\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mComponent-headline-0-2-110\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# iterates over each link and extracts the href and title\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m link \u001B[38;5;129;01min\u001B[39;00m soup\u001B[38;5;241m.\u001B[39mfind_all(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ma\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mComponent-headline-0-2-110\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "\u001B[1;31mNameError\u001B[0m: name 'soup' is not defined"
     ]
    }
   ],
   "source": [
    "# EXAMPLE 1\n",
    "\n",
    "# returns all `a` links that also contain `Component-headline-0-2-110`\n",
    "soup.find_all(\"a\", \"Component-headline-0-2-110\")\n",
    "\n",
    "# iterates over each link and extracts the href and title\n",
    "for link in soup.find_all(\"a\", \"Component-headline-0-2-110\"):\n",
    "    url = \"www.apnews.com\" + link['href']\n",
    "    title = link.text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As mentioned in the official documentation [here](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#attributes) and [here](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#the-keyword-arguments), a tag (such as `a`) may have many attributes, and you can search them by putting your terms in a dictionary."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# EXAMPLE 2\n",
    "# this returns the same exact subset of links as the example above\n",
    "# so, we could iterate through the list just like above\n",
    "soup.find_all(\"a\", attrs={\"data-key\": \"card-headline\"})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Alternatively, we could use Regular Expressions if we were confident that our Regex pattern only matched on the relevant links."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'home_page' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [5]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# EXAMPLE 3\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# instead of using the BeautifulSoup, we are handling all of the parsing\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# ourselves, and working directly with the original Requests text\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m re\u001B[38;5;241m.\u001B[39mfindall(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<a class=\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;124mComponent-headline.*?href=\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;124m(.+?)\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;124m><h1.+?>(.+?)</h1></a>\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[43mhome_page\u001B[49m\u001B[38;5;241m.\u001B[39mtext)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'home_page' is not defined"
     ]
    }
   ],
   "source": [
    "# EXAMPLE 3\n",
    "# instead of using the BeautifulSoup, we are handling all of the parsing\n",
    "# ourselves, and working directly with the original Requests text\n",
    "re.findall(\"<a class=\\\"Component-headline.*?href=\\\"(.+?)\\\"><h1.+?>(.+?)</h1></a>\", home_page.text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}